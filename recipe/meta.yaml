{% set name = "tokenizers" %}
{% set version = "0.20.1" %}

package:
  name: {{ name|lower }}
  version: {{ version }}

source:
  url: https://pypi.io/packages/source/{{ name[0] }}/{{ name }}/{{ name }}-{{ version }}.tar.gz
  sha256: 84edcc7cdeeee45ceedb65d518fffb77aec69311c9c8e30f77ad84da3025f002
  patches:  # [win]
    - remove-multiprocess-fork.patch  # [win]

build:
  number: 1
  skip: True  # [py<37]
  missing_dso_whitelist:
    - /usr/lib/libresolv.9.dylib  # [osx]
    - /usr/lib64/libgcc_s.so.1    # [linux]
    - $RPATH/ld64.so.1            # [s390x]
  script:
    - {{ PYTHON }} -m pip install . --no-deps --no-build-isolation -vv

requirements:
  build:
    - {{ compiler('cxx') }}
    - {{ compiler('rust') }}
    # It's needed to find openssl
    - pkg-config
    - m2-patch  # [win]
  host:
    - python
    - pip
    - openssl {{ openssl }}  # [linux]
    - maturin >=1.0,<2.0
  run:
    - python
    - openssl  # [linux]
    - huggingface_hub >=0.16.4,<1.0

test:
  source_files:
    - bindings/python/tests
    - bindings/python/conftest.py
  imports:
    - tokenizers
    - tokenizers.models
    - tokenizers.decoders
    - tokenizers.normalizers
    - tokenizers.pre_tokenizers
    - tokenizers.processors
    - tokenizers.trainers
    - tokenizers.implementations
    - tokenizers.tools
    - huggingface_hub
  commands:
    - pip check
    - cd bindings/python
    # tests require multiprocessing with fork, which is not available on Windows.
    - pytest tests --ignore="tests/documentation" -k "not test_continuing_prefix_trainer_mistmatch"  # [not win]
    - pytest tests --ignore="tests/documentation" -k "not (test_continuing_prefix_trainer_mistmatch or test_multiprocessing_with_parallelism)" # [win]
  requires:
    - pip
    - pytest
    - requests
    - numpy
    - datasets

about:
  home: https://github.com/huggingface/tokenizers
  license: Apache-2.0
  license_family: Apache
  license_file: LICENSE
  summary: Fast State-of-the-Art Tokenizers optimized for Research and Production
  description: |
    Provides an implementation of today's most used tokenizers, with a focus on 
    performance and versatility.
  dev_url: https://github.com/huggingface/tokenizers
  doc_url: https://huggingface.co/docs/tokenizers/index

extra:
  recipe-maintainers:
    - anthchirp
    - hadim
    - ndmaxar
    - oblute
    - setu4993
